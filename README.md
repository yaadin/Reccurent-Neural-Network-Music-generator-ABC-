## Music Generation with LSTM

# Dataset

The project uses a **text-based music dataset** (`songs.txt`) in ABC notation.  

Each character (note, symbol) is treated as a token.  

A vocabulary of unique tokens is built for encoding and decoding.

# Data Pipeline

- Text is **vectorized** using a character-to-index mapping.
  
- Batches of sequences are created with a fixed `seq_length` for training.
  
- Input sequences are paired with target sequences offset by one character for next-token prediction.

# Model

The architecture is a **character-level LSTM**:

- **Embedding layer** maps tokens to dense vectors
- 
- **LSTM layer** memory and sequence notation
- 
- **Fully connected layer** outputs logits over the vocabulary

# Training

- Optimizer: **Adam**
  
- Loss: **Cross-entropy** between predicted and target tokens
  
- Mini-batches of sequences are fed through the LSTM
  
- Training uses a **checkpointing system** to save model weights
  
- CometML is integrated to **log metrics and parameters** during training

#Inference

After training, new music sequences are generated by:

1. Encoding a start string into token indices  
2. Feeding tokens into the trained LSTM with hidden states  
3. Sampling from the output probability distribution at each step  
4. Converting sampled indices back into characters  

The generated music is saved in `generated_songs.abc`.



